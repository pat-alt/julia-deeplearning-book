[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julia for Deep Learning",
    "section": "",
    "text": "Preface\nThis book provides an overview of a pure Julia implementation of the fastai book: Deep Learning for Coders with fastai and PyTorch. The original book works with Python and PyTorch. This book works with Julia and relies primarily on Flux.jl (Innes 2018). The original book is written by Jeremy Howard and Sylvain Gugge (Howard and Gugger 2020). This book is written by participants of the Julia for Deep Learning course.\nThe book is served from GitHub.\n\n\n\n\nHoward, Jeremy, and Sylvain Gugger. 2020. Deep Learning for Coders with Fastai and PyTorch. \"O’Reilly Media, Inc.\".\n\n\nInnes, Mike. 2018. “Flux: Elegant Machine Learning with Julia.” Journal of Open Source Software 3 (25): 602."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "TBD"
  },
  {
    "objectID": "chapters/c4.html#computing-metrics-using-broadcasting",
    "href": "chapters/c4.html#computing-metrics-using-broadcasting",
    "title": "2  Chapter 4: Under the Hood: Training a Digit Classifier",
    "section": "2.1 Computing Metrics Using Broadcasting",
    "text": "2.1 Computing Metrics Using Broadcasting\nThe rule of broadcasting in Julia is different from Python? In Python, first align all dimensions to the right, then broadcast. In Julia, first align all dimensions to the left, then broadcast. So in python [1000, 28, 28] - [28, 28] is allowed, but in Julia, we need [28, 28, 1000] - [28, 28]. Use permutedims to change the order of dimensions.\n\nvalid_threes = sort(readdir(joinpath(data_path, \"test\", \"3\")))\nvalid_3_tens = stack([load(joinpath(data_path, \"test\", \"3\", img)) for img in valid_threes])\n\nvalid_sevens = sort(readdir(joinpath(data_path, \"test\", \"7\")))\nvalid_7_tens = stack([load(joinpath(data_path, \"test\", \"7\", img)) for img in valid_sevens])\n\n# valid_3_tens = permutedims(valid_3_tens, [3, 1, 2])\n\nsize(valid_3_tens), size(valid_7_tens)\n\n((28, 28, 1010), (28, 28, 1028))\n\n\n\nfunction mnist_distance(a, b)\n    mm = mean(Float32.(abs.(a .- b)), dims=(1, 2))\n    return dropdims(mm, dims=(1, 2))\nend\n\nmnist_distance(a_3, mean3)[1]\n\n0.12612005f0\n\n\n\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\n(size(valid_3_dist), valid_3_dist)\n\n((1010,), Float32[0.12803426, 0.16230617, 0.12421783, 0.14686641, 0.120200165, 0.118782386, 0.16995831, 0.12664512, 0.13367367, 0.11829293  …  0.12947088, 0.1525875, 0.1556588, 0.13255748, 0.13654083, 0.17447978, 0.12789737, 0.15078008, 0.12630658, 0.12596397])\n\n\n\nsize(valid_3_tens .- mean3)\n\n(28, 28, 1010)\n\n\n\nis_3(x) = mnist_distance(x, mean3) .&lt; mnist_distance(x, mean7)\n\nis_3 (generic function with 1 method)\n\n\n\nis_3(a_3)\nis_3(valid_3_tens[:, :, 1:10])\n\nis_3(valid_7_tens[:, :, 1:10])\n\n10-element BitVector:\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n\n\n\naccuracy_3s = mean(is_3(valid_3_tens))\naccuracy_7s = mean(1 .- is_3(valid_7_tens))\n\naccuracy_3s, accuracy_7s\n\n(0.9168316831683169, 0.9854085603112841)"
  },
  {
    "objectID": "chapters/c4.html#calculating-gradients",
    "href": "chapters/c4.html#calculating-gradients",
    "title": "2  Chapter 4: Under the Hood: Training a Digit Classifier",
    "section": "2.2 Calculating Gradients",
    "text": "2.2 Calculating Gradients\n\n2.2.1 Using Flux.jl\nTaking gradients in Flux.jl is as simple as calling gradient on a function. For example, to take the gradient of f(x) = x^2 at x = 2, we can do the following:\n\nf(x) = x^2\ndf(x) = gradient(f, x)[1]\ndf(2)\n\n4.0\n\n\nBelow we implement and visualise gradient descent from scratch in Julia.\n\nxmax = 10\nn = 100\nplt = plot(\n    range(-xmax, xmax, length=n), f;\n    label=\"f(x)\", lw=5, xlim=1.5 .* [-xmax, xmax],\n    xlab=\"Parameter\", ylab=\"Loss\", legend=false\n)\n\nnsteps = 10\nlrs = [0.05, 0.3, 0.975, 1.025]\ndescend(x; lr=0.1) = x - lr * df(x)\nx = [-0.75xmax]\nx = repeat(x, length(lrs), 1)                             # repeat x for each learning rate\nplts = [deepcopy(plt) for i in 1:length(lrs)]           # repeat plt for each learning rate\nanim = @animate for j in 1:nsteps\n    global x = hcat(x, zeros(size(x, 1)))                # add column of zeros to x\n    for (i, lr) in enumerate(lrs)\n        _plt = plot(plts[i], title=\"lr = $lr\", ylims=(0, f(xmax)), legend=false)\n        scatter!([x[i, j]], [f(x[i, j])]; label=nothing, ms=5, c=:red)    # plot current point\n        x[i, j+1] = descend(x[i, j]; lr=lr)                               # descend\n        Δx = x[i, j+1] - x[i, j]\n        Δy = f(x[i, j+1]) - f(x[i, j])\n        quiver!([x[i, j]], [f(x[i, j])], quiver=([Δx], [0]), c=:red)          # horizontal arrow\n        quiver!([x[i, j+1]], [f(x[i, j])], quiver=([0], [Δy]), c=:red)        # vertical arrow\n        plts[i] = _plt\n    end\n    plot(\n        plts..., legend=false,\n        plot_title=\"Step $j\", margin=5mm,\n        dpi=300,\n    )\nend\ngif(anim, joinpath(www_path, \"c4_gd.gif\"), fps=0.5)\n\n\n\n\nFigure 2.1: Gradient descent for different learning rates"
  },
  {
    "objectID": "chapters/c4.html#an-end-to-end-sgd-example",
    "href": "chapters/c4.html#an-end-to-end-sgd-example",
    "title": "2  Chapter 4: Under the Hood: Training a Digit Classifier",
    "section": "2.3 An End-to-End SGD Example",
    "text": "2.3 An End-to-End SGD Example\n\n## is time a good variable name?\ntime = collect(range(start=0, stop=19))\n\nspeed = @. $rand(20) + 0.75 * (time - 9.5)^2 + 1\n\nscatter(time, speed, legend=false, xlabel=\"time\", ylabel=\"speed\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction f(t, params)\n    a, b, c = params\n    return @. a * (t - b)^2 + c\nend\n\nfunction mse(preds, targets)\n    return sum((preds .- targets) .^ 2) / length(preds)\nend\n\nmse (generic function with 1 method)\n\n\n\nfunction show_preds(preds)\n    scatter(time, speed)\n    scatter!(time, preds, color=\"red\")\nend\n\nshow_preds (generic function with 1 method)\n\n\n\nparams = rand(3)\npreds = f(time, params)\n\nshow_preds(preds)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloss = mse(preds, speed)\n\n19963.096411545597\n\n\n\ndloss(params) = gradient(params -&gt; mse(f(time, params), speed), params)\n\ngrad = dloss(params)[1]\n\nlr = 1e-5\nparams = params .- lr .* grad\n\npreds = f(time, params)\nmse(preds, speed)\n\nshow_preds(preds)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## params will be updated in place\nfunction apply_step!(params; lr=1e-5, prn=true)\n    grad = dloss(params)[1]\n    params .-= lr * grad ## inplace update\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    if prn\n        println(loss)\n        println(grad)\n        println(params)\n    end\n    return preds\nend\n\napply_step! (generic function with 1 method)\n\n\n\nparams = rand(3)\nplts = []\n\nfor i in range(1, 4)\n    push!(plts, show_preds(apply_step!(params; lr=0.0001, prn=false)))\nend\n\nplot(\n    plts..., legend=false,\n    plot_title=\"First four steps\", margin=5mm,\n    dpi=300,\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\nparams = rand(3)\npreds = f(time, params)\n\nplts = []\npush!(plts, show_preds(preds))\n\nlr = 0.0001  ## how to adjust learning rate? takes a lot of time to learn\nfor i in range(0, 60000)\n    apply_step!(params, prn=false)\nend\n\npreds = apply_step!(params, prn=true);\npush!(plts, show_preds(preds))\n\nplot(\n    plts..., legend=false,\n    plot_title=\"After 60000 steps\", margin=5mm,\n    dpi=300,\n)\n\n1.9494954912048588\n[-0.030622611058086946, -5.9133689834345304e-5, 1.826231359046217]\n[0.717719488068244, 9.497674832877456, 3.5108245061691448]"
  },
  {
    "objectID": "chapters/c4.html#the-mnist-loss-function",
    "href": "chapters/c4.html#the-mnist-loss-function",
    "title": "2  Chapter 4: Under the Hood: Training a Digit Classifier",
    "section": "2.4 The MNIST Loss Function",
    "text": "2.4 The MNIST Loss Function\n\ntrain_x = cat(stacked_threes, stacked_sevens, dims=3) |&gt; x -&gt; reshape(x, 28 * 28, :) |&gt; transpose;\ntrain_y = vcat(repeat([1], size(stacked_threes)[3]), repeat([0], size(stacked_sevens)[3]));\n\nsize(train_x), size(train_y)\n\n((12396, 784), (12396,))\n\n\n\ndset = [(train_x[i, :], train_y[i]) for i in range(1, size(train_x)[1])]\nx, y = dset[1]\nsize(dset), size(x), y\n\n((12396,), (784,), 1)\n\n\n\nvalid_x = cat(valid_3_tens, valid_7_tens, dims=3) |&gt; x -&gt; reshape(x, 28 * 28, :) |&gt; transpose;\nvalid_y = vcat(repeat([1], size(valid_3_tens)[3]), repeat([0], size(valid_7_tens)[3]));\nvalid_dset = zip(eachrow(valid_x), valid_y);\n\nsize(valid_x), size(valid_y), size(valid_dset)\n\n((2038, 784), (2038,), (2038,))\n\n\n\ninit_params(size; std=1.0) = randn(size) * std\n\nweights = init_params((28 * 28, 1))\n\nbias = init_params(1)\n\nsize(weights), size(bias)\n\n((784, 1), (1,))\n\n\n\ntrain_x = convert(Array{Float32}, train_x)\n\ntrain_x[1:1, :] * weights .+ bias\n\n1×1 Matrix{Float64}:\n -6.929789202129583\n\n\nPytorch tensor provides a tag to indicate if gradient is to be computed. This is not needed in Flux? To get gradient, just use gradient function in Flux\n\ngradient(weights -&gt; sum(train_x[1:1, :] * weights), weights)\n\n([0.0; 0.0; … ; 0.0; 0.0;;],)\n\n\n\nlinear1(xb) = xb * weights .+ bias\npreds = linear1(train_x)\n\n12396×1 Matrix{Float64}:\n  -6.929789202129595\n -14.83395379293493\n  -4.358113057095578\n  -2.589705111776829\n  -3.750241840626214\n   2.8752630304879627\n  -2.6997894959835333\n  -4.373476673167225\n  -5.82727906506578\n  -4.870298217144499\n  -5.332426344363814\n -11.057632013627192\n  -1.1523106816373356\n   ⋮\n   9.717395500537705\n -11.240414584565805\n -10.271768015182378\n  -7.651002665208998\n   2.8431862103775147\n   8.166721065755432\n  12.288306649312544\n   9.68608450233336\n  -0.6859069176910312\n  -4.462807730907561\n  -8.639962766925642\n   3.9366085678505316\n\n\n\ncorrects = (preds .&gt; 0.0) .=== Bool.(train_y)\n\nmean(corrects)\n\n0.3844788641497257\n\n\n\nweights[1] *= 1.0001\n\npreds = linear1(train_x)\nmean((preds .&gt; 0.0) .== Bool.(train_y))\n\n0.3844788641497257\n\n\n\ntrgts = [1, 0, 1]\nprds = [0.9, 0.4, 0.2]\n\nmnist_loss(predictions, targets) = mean(t === 1 ? 1 - p : p for (p, t) in zip(predictions, targets))\n\nmnist_loss(prds, trgts), mnist_loss([0.9, 0.4, 0.8], trgts)\n\n(0.43333333333333335, 0.2333333333333333)\n\n\n\nsigmoid(x) = 1 / (1 + exp(-x))\n\nprint(sigmoid.(rand(10)))\n\nplot(range(-5, 5, length=100), sigmoid)\n\n[0.5355998417823861, 0.5195607373574936, 0.6471942826866892, 0.6620458897404772, 0.5304832905623063, 0.6860838674407848, 0.7128645571899711, 0.6183204894499842, 0.5373823177481695, 0.6088463974899412]\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction mnist_loss(predictions, targets)\n    predictions = sigmoid.(predictions)\n    return mean([t === 1 ? 1 - p : p for (p, t) in zip(predictions, targets)])\nend\n\nmnist_loss (generic function with 1 method)"
  },
  {
    "objectID": "chapters/c4.html#sgd-and-mini-batches",
    "href": "chapters/c4.html#sgd-and-mini-batches",
    "title": "2  Chapter 4: Under the Hood: Training a Digit Classifier",
    "section": "2.5 SGD and Mini-Batches",
    "text": "2.5 SGD and Mini-Batches\n\ncoll = range(1, 15)\n\ndl = DataLoader((coll), batchsize=5, shuffle=true)\n\ncollect(dl)\n\n3-element Vector{Vector{Int64}}:\n [7, 14, 4, 10, 6]\n [15, 9, 11, 8, 2]\n [1, 5, 13, 3, 12]\n\n\n\nlowercase_alphabets = 'a':'z' ## [Char(i) for i in 97:122]\n\nds = [ (i, v) for (i, v) in enumerate(lowercase_alphabets)]\n\ndl = DataLoader(ds, batchsize=5, shuffle=true)\ncollect(dl)\n\n6-element Vector{Vector{Tuple{Int64, Char}}}:\n [(8, 'h'), (22, 'v'), (24, 'x'), (21, 'u'), (2, 'b')]\n [(23, 'w'), (1, 'a'), (3, 'c'), (19, 's'), (16, 'p')]\n [(6, 'f'), (15, 'o'), (20, 't'), (9, 'i'), (17, 'q')]\n [(12, 'l'), (7, 'g'), (13, 'm'), (11, 'k'), (4, 'd')]\n [(26, 'z'), (25, 'y'), (18, 'r'), (5, 'e'), (10, 'j')]\n [(14, 'n')]\n\n\nDoes dataloader work with files and directories?\n\nweights = init_params((28*28,1))\nbias = init_params(1)\nsize(weights), size(bias)\n\n((784, 1), (1,))\n\n\n\nfunction reformat_dl(d1) \n    xb = stack([x for (x, y) in d1], dims=1)\n    yb = stack([[y] for (x, y) in d1], dims=1)\n    return xb, yb\nend\n\ndl = DataLoader(dset, batchsize=256, shuffle=true)\n\nd1 = first(dl)\nlength(d1)\n\nxb, yb = reformat_dl(d1)\n\nsize(xb), size(yb)\n\n((256, 784), (256, 1))\n\n\n\nvalid_x = convert(Array{Float32}, valid_x)\n\nvalid_dset = [(valid_x[i, :], valid_y[i]) for i in range(1, size(valid_x)[1])]\n\nvalid_dl = DataLoader(valid_dset, batchsize=256, shuffle=true)\n\n8-element DataLoader(::Vector{Tuple{Vector{Float32}, Int64}}, shuffle=true, batchsize=256)\n  with first element:\n  256-element Vector{Tuple{Vector{Float32}, Int64}}\n\n\n\nbatch = train_x[1:4, :]\nsize(batch)\n\npreds = linear1(batch)\n\nloss = mnist_loss(preds, train_y[1:4])\n\n## redefine linear layer to include weights and bias as parameters\n\nlinear1(xb, weights, bias) = xb * weights .+ bias\npreds = linear1(batch, weights, bias)\n\ncurr_gr = gradient(weights, bias) do weights, bias\n    preds = linear1(batch, weights, bias)\n    mnist_loss(preds, train_y[1:4])\nend\n\n([0.0; 0.0; … ; 0.0; 0.0;;], [-0.0623355176812617])\n\n\n\n# using dictionary to store parameters\n\nparams = Dict(\"weights\" =&gt; weights, \"bias\" =&gt; bias)\n\nlinear1(xb, params) = xb * params[\"weights\"] .+ params[\"bias\"]\n\ncurr_gr = gradient(params) do params\n    preds = linear1(batch, params)\n    mnist_loss(preds, train_y[1:4])\nend\n\n(Dict{Any, Any}(\"weights\" =&gt; [0.0; 0.0; … ; 0.0; 0.0;;], \"bias\" =&gt; [-0.0623355176812617]),)\n\n\n\nlr = 1e-4\nfunction calc_grad(xb, yb, model, weights, bias)\n    preds = model(xb, weights, bias)\n    loss = mnist_loss(preds, yb)\n    curr_gr = gradient(weights, bias) do weights, bias\n        preds = model(xb, weights, bias)\n        mnist_loss(preds, yb)\n    end\nend\n\ncalc_grad (generic function with 1 method)\n\n\nUsing params dictionary.\n\nfunction calc_grad(xb, yb, model, params)\n    preds = model(xb, params)\n    loss = mnist_loss(preds, yb)\n    curr_gr = gradient(params) do params\n        preds = model(xb, params)\n        mnist_loss(preds, yb)\n    end\nend\n\ncalc_grad (generic function with 2 methods)\n\n\n\ncurr_grad = calc_grad(batch, train_y[1:4], linear1, weights, bias)\ndict_grad = calc_grad(batch, train_y[1:4], linear1, params)[1]\n## weights.grad.mean(),bias.grad\n\nmean(curr_grad[1]), mean(curr_grad[2])\nmean(dict_grad[\"weights\"]), mean(dict_grad[\"bias\"])\n\n(-0.011187652961387788, -0.0623355176812617)\n\n\n\nfunction train_epoch(model, lr, params)\n    for dd in dl\n        xb, yb = reformat_dl(dd)\n        grad = calc_grad(xb, yb, model, params)[1]\n        for k in keys(params)\n            params[k] .-= grad[k] * lr\n            ## no need to zero_grad? in Pytorch, p.grad.zero_()\n        end\n    end\nend\n\ntrain_epoch(linear1, lr, params)\n\n\n(preds .&gt; 0.0) == Bool.(train_y[1:4])\n\nfalse\n\n\n\nfunction batch_accuracy(xb, yb)\n    preds = sigmoid.(xb)\n    correct = (preds .&gt; 0.5) .== yb\n    return mean(correct)\nend\n\nbatch_accuracy(linear1(batch, params), train_y[1:4])\n\n0.75\n\n\n\nfunction validate_epoch(model)\n    accs = zeros(length(valid_dl))\n    i = 1\n    for dd in valid_dl\n        xb, yb = reformat_dl(dd)\n        accs[i] = batch_accuracy(model(xb, params), yb)\n        i = i + 1\n    end\n    return round(mean(accs), digits=4)\nend\n\nfunction train_accuracy(model)\n    accs = zeros(length(dl))\n    i = 1\n    for dd in dl\n        xb, yb = reformat_dl(dd)\n        accs[i] = batch_accuracy(model(xb, params), yb)\n        i = i + 1\n    end\n    return round(mean(accs), digits=4)\nend\n\ntrain_accuracy (generic function with 1 method)\n\n\n\nlr = 1\n\nweights = init_params((28 * 28, 1))\nbias = init_params(1)\n\nparams = Dict(\"weights\" =&gt; weights, \"bias\" =&gt; bias)\n\ntrain_epoch(linear1, lr, params)\n\nvalidate_epoch(linear1)\n\n0.9253\n\n\n\nfor i in range(1, 20)\n    train_epoch(linear1, lr, params)\n    println((i, validate_epoch(linear1), train_accuracy(linear1)))\nend\n\n(1, 0.9455, 0.9482)\n(2, 0.9524, 0.9568)\n(3, 0.9563, 0.961)\n(4, 0.9593, 0.9637)\n(5, 0.9617, 0.967)\n(6, 0.9641, 0.9701)\n(7, 0.9657, 0.9714)\n(8, 0.9666, 0.973)\n(9, 0.9676, 0.9745)\n(10, 0.9706, 0.9754)\n(11, 0.9715, 0.9763)\n(12, 0.9721, 0.9767)\n(13, 0.973, 0.9774)\n(14, 0.9719, 0.9783)\n(15, 0.9725, 0.9789)\n(16, 0.9725, 0.9791)\n(17, 0.9755, 0.9798)\n(18, 0.974, 0.9796)\n(19, 0.9749, 0.9803)\n(20, 0.975, 0.9804)"
  },
  {
    "objectID": "chapters/c4.html#creating-an-optimizer",
    "href": "chapters/c4.html#creating-an-optimizer",
    "title": "2  Chapter 4: Under the Hood: Training a Digit Classifier",
    "section": "2.6 Creating an Optimizer",
    "text": "2.6 Creating an Optimizer\nA Flux based implementation\n\nmodel = Chain(\n    Dense(28 * 28 =&gt; 1),\n    Flux.sigmoid  ## or σ\n)\n\noptim = Flux.setup(Flux.Adam(1.0), model)\n\nlosses = []\n\nfor epoch in 1:20\n    for dd in dl\n        xb, yb = reformat_dl(dd)\n        loss, grads = Flux.withgradient(model) do m\n            # Evaluate model and loss inside gradient context:\n            y_hat = m(xb')\n            Flux.binarycrossentropy(y_hat, yb')  # mnist_loss(y_hat', yb)\n        end\n        Flux.update!(optim, model, grads[1])\n        push!(losses, loss)  # logging, outside gradient context\n    end\nend\n\noptim # parameters, momenta and output have all changed\n\nxb, yb = reformat_dl(first(valid_dl))\n\nout2 = model(xb')  # first row is prob. of true, second row p(false)\n\nmean((out2[1, :] .&gt; 0.5) .== yb)\n\n0.9765625\n\n\nShow examples of predicting seven and three.\n\nxb, yb = reformat_dl(collect(valid_dl)[end])\n\nseven_examples = rand(findall(y -&gt; y == 0, yb[:]), 9)\n\nconvert(Array{Gray}, mosaic(map(i -&gt; reshape(xb[i, :], 28, 28), seven_examples), ncol=3))\n\n[b &gt; 0.5 ? \"three\" : \"seven\" for b in model(xb[seven_examples, :]')]\n\n1×9 Matrix{String}:\n \"seven\"  \"seven\"  \"seven\"  \"seven\"  …  \"seven\"  \"seven\"  \"seven\"  \"seven\"\n\n\n\nthree_examples = rand(findall(y -&gt; y == 1, yb[:]), 9)\nconvert(Array{Gray}, mosaic(map(i -&gt; reshape(xb[i, :], 28, 28), three_examples), ncol=3))\n[b &gt; 0.5 ? \"three\" : \"seven\" for b in model(xb[three_examples, :]')]\n\n1×9 Matrix{String}:\n \"three\"  \"three\"  \"three\"  \"three\"  …  \"three\"  \"three\"  \"three\"  \"three\""
  },
  {
    "objectID": "chapters/c4.html#adding-a-nonlinearity",
    "href": "chapters/c4.html#adding-a-nonlinearity",
    "title": "2  Chapter 4: Under the Hood: Training a Digit Classifier",
    "section": "2.7 Adding a Nonlinearity",
    "text": "2.7 Adding a Nonlinearity\n\nfunction simple_net1(xb)\n    res = xb * w1 .+ b1'\n    res[res.&lt;0] .= 0\n    res = res * w2 .+ b2\n    return res\nend\n\nw1 = init_params((28 * 28, 30))\nb1 = init_params(30)\nw2 = init_params((30, 1))\nb2 = init_params(1)\n\nsimple_net1(train_x[1:4, :])\n\n4×1 Matrix{Float64}:\n  20.732784720636907\n  42.26200931546502\n -11.294101896847437\n  -9.426694039389105\n\n\n\nplot(range(-5, 5), Flux.relu)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsimple_net_flux = Chain(\n    Flux.Dense(28 * 28, 30),\n    Flux.relu,\n    Flux.Dense(30, 1)\n)\n\nFlux.params(simple_net_flux[1])[1] .= w1'\nFlux.params(simple_net_flux[1])[2] .= b1\n\nFlux.params(simple_net_flux[3])[1] .= w2'\nFlux.params(simple_net_flux[3])[2] .= b2\n\nsimple_net_flux(train_x[1:4, :]')\n\n1×4 Matrix{Float32}:\n 20.7328  42.262  -11.2941  -9.42669"
  },
  {
    "objectID": "chapters/c4.html#training-a-digit-classifier",
    "href": "chapters/c4.html#training-a-digit-classifier",
    "title": "2  Chapter 4: Under the Hood: Training a Digit Classifier",
    "section": "2.8 Training a Digit Classifier",
    "text": "2.8 Training a Digit Classifier\nThe MNIST dataset can be loaded in Julia as follows:\n\n# Data\nX, y = MLDatasets.MNIST(:train)[:]\ny_enc = Flux.onehotbatch(y, 0:9)\nXtest, ytest = MLDatasets.MNIST(:test)[:]\nytest_enc = onehotbatch(ytest, 0:9)\nmosaic(map(i -&gt; convert2image(MNIST, X[:, :, i]), rand(1:60000, 100)), ncol=10)\n\n\n\n\nWe can preprocess the data as follows:\n\ni_train, i_val = [], []\nfor (k, v) in group_indices(y)\n    _i_train, _i_val = splitobs(v, at=0.7)\n    push!(i_train, _i_train...)\n    push!(i_val, _i_val...)\nend\nXtrain, ytrain = X[:, :, i_train], y_enc[:, i_train]\nXval, yval = X[:, :, i_val], y_enc[:, i_val]\n\n([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Bool[0 0 … 0 0; 0 0 … 1 1; … ; 0 0 … 0 0; 0 0 … 0 0])\n\n\nNext, we define a data loader:\n\nbatchsize = 128\ntrain_set = DataLoader((Xtrain, ytrain), batchsize=batchsize, shuffle=true)\nval_set = DataLoader((Xval, yval), batchsize=batchsize)\n\n141-element DataLoader(::Tuple{Array{Float32, 3}, OneHotMatrix{UInt32, Vector{UInt32}}}, batchsize=128)\n  with first element:\n  (28×28×128 Array{Float32, 3}, 10×128 OneHotMatrix(::Vector{UInt32}) with eltype Bool,)\n\n\nWe can now define a model, based on how we preprocessed the data:\n\nmodel = Chain(\n    Flux.flatten,\n    Dense(28^2, 32, relu),\n    Dense(32, 10),\n    softmax\n)\n\n\nChain(\n  Flux.flatten,\n  Dense(784 =&gt; 32, relu),               # 25_120 parameters\n  Dense(32 =&gt; 10),                      # 330 parameters\n  NNlib.softmax,\n)                   # Total: 4 arrays, 25_450 parameters, 99.664 KiB.\n\n\n\nFinally, what’s left to do is to define a loss function and an optimiser:\n\nloss(y_hat, y) = Flux.Losses.crossentropy(y_hat, y)\nopt_state = Flux.setup(Adam(), model)\n\nBefore we start training, we define some helper functions:\n\n# Callbacks:\nfunction accuracy(model, data::DataLoader)\n    acc = 0\n    for (x, y) in data\n        acc += sum(onecold(model(x)) .== onecold(y)) / size(y, 2)\n    end\n    return acc / length(data)\nend\n\nfunction avg_loss(model, data::DataLoader)\n    _loss = 0\n    for (x, y) in data\n        _loss += loss(model(x), y)[1]\n    end\n    return _loss / length(data)\nend\n\nAs a very last step, we set up our training logs:\n\n# Final setup:\nnepochs = 100\nacc_train, acc_val = accuracy(model, train_set), accuracy(model, val_set)\nloss_train, loss_val = avg_loss(model, train_set), avg_loss(model, val_set)\n\nlog = DataFrame(\n    epoch=0,\n    acc_train=acc_train,\n    acc_val=acc_val,\n    loss_train=loss_train,\n    loss_val=loss_val\n)\n\nBelow we finally train our model:\n\n# Training loop:\nfor epoch in 1:nepochs\n\n    for (i, data) in enumerate(train_set)\n\n        # Extract data:\n        input, label = data\n\n        # Compute loss and gradient:\n        val, grads = Flux.withgradient(model) do m\n            result = m(input)\n            loss(result, label)\n        end\n\n        # Detect loss of Inf or NaN. Print a warning, and then skip update!\n        if !isfinite(val)\n            @warn \"loss is $val on item $i\" epoch\n            continue\n        end\n\n        Flux.update!(opt_state, model, grads[1])\n\n    end\n\n    # Monitor progress:\n    acc_train, acc_val = accuracy(model, train_set), accuracy(model, val_set)\n    loss_train, loss_val = avg_loss(model, train_set), avg_loss(model, val_set)\n    results = Dict(\n        :epoch =&gt; epoch,\n        :acc_train =&gt; acc_train,\n        :acc_val =&gt; acc_val,\n        :loss_train =&gt; loss_train,\n        :loss_val =&gt; loss_val\n    )\n    push!(log, results)\n\n    # Print progress:\n    vals = Matrix(results_df[2:end,[:loss_train,:loss_val]])\n    plt = UnicodePlots.lineplot(1:epoch, vals; \n        name=[\"Train\",\"Validation\"], title=\"Loss in epoch $epoch\", xlim=(1,nepochs))\n    UnicodePlots.display(plt)\n\nend\n\nFigure 2.2 shows the training and validation loss and accuracy over epochs. The model is overfitting, as the validation loss increases after bottoming out at around epoch 20.\n\noutput = DataFrame(log)\noutput = output[2:end, :]\n\nanim = @animate for epoch in 1:maximum(output.epoch)\n    p_loss = plot(output[1:epoch, :epoch], Matrix(output[1:epoch, [:loss_train, :loss_val]]),\n        label=[\"Train\" \"Validation\"], title=\"Loss\", legend=:topleft)\n    p_acc = plot(output[1:epoch, :epoch], Matrix(output[1:epoch, [:acc_train, :acc_val]]),\n        label=[\"Train\" \"Validation\"], title=\"Accuracy\", legend=:topleft)\n    plot(p_loss, p_acc, layout=(1, 2), dpi=300, margin=5mm, size=(800, 400))\nend\ngif(anim, joinpath(www_path, \"c4_mnist.gif\"), fps=5)\n\n\n\n\nFigure 2.2: Training and validation loss and accuracy"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "contributors.html#local-rendering-instructions",
    "href": "contributors.html#local-rendering-instructions",
    "title": "4  Contributors’ Guide",
    "section": "4.1 Local rendering instructions",
    "text": "4.1 Local rendering instructions\nBy default, the book is rendered automatically through GitHub actions when you merge to main. However, if you want to preview the book locally, you can do so by following these instructions:\n\nClone the repo for this book somewhere on your computer. Navigate using a terminal to the folder containing the repository.\nAdd the Julia kernel to Jupyter. This is currently tested with Julia 1.9, your results may vary with different versions of julia.\n# Load the current environment\nusing Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n\n# Import IJulia\nusing IJulia\n\n# Call a notebook. You can close it once you call this --\n# we only need to run notebook() once for it to build the\n# jupyter kernel for Julia.\nnotebook()\n\n\n\n\n\n\n\nTroubleshooting\n\n\n\nIf you get an error message about jupyter not being found, you may need to build IJulia first. This\n\n\n\nInstall Quarto.\nRender the website from the root of this folder with\nquarto render\nOptionally, if you want to preview the HTML version of the page before you merge it to main, you can run\nquarter preview"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Howard, Jeremy, and Sylvain Gugger. 2020. Deep Learning\nfor Coders with Fastai and PyTorch.\n\"O’Reilly Media, Inc.\".\n\n\nInnes, Mike. 2018. “Flux: Elegant Machine Learning\nwith Julia.” Journal of Open Source\nSoftware 3 (25): 602."
  }
]